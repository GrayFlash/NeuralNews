{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_frequency_table(text_string) -> dict:\n",
    "\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text_string)\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    freqTable = dict()\n",
    "    for word in words:\n",
    "        word = ps.stem(word)\n",
    "        if word in stopWords:\n",
    "            continue\n",
    "        if word in freqTable:\n",
    "            freqTable[word] += 1\n",
    "        else:\n",
    "            freqTable[word] = 1\n",
    "\n",
    "    return freqTable\n",
    "\n",
    "\n",
    "def _score_sentences(sentences, freqTable) -> dict:\n",
    "    sentenceValue = dict()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        word_count_in_sentence = (len(word_tokenize(sentence)))\n",
    "        word_count_in_sentence_except_stop_words = 0\n",
    "        for wordValue in freqTable:\n",
    "            if wordValue in sentence.lower():\n",
    "                word_count_in_sentence_except_stop_words += 1\n",
    "                if sentence[:10] in sentenceValue:\n",
    "                    sentenceValue[sentence[:10]] += freqTable[wordValue]\n",
    "                else:\n",
    "                    sentenceValue[sentence[:10]] = freqTable[wordValue]\n",
    "\n",
    "        if sentence[:10] in sentenceValue:\n",
    "            sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] / word_count_in_sentence_except_stop_words\n",
    "    return sentenceValue\n",
    "\n",
    "\n",
    "def _find_average_score(sentenceValue) -> int:\n",
    "\n",
    "    sumValues = 0\n",
    "    for entry in sentenceValue:\n",
    "        sumValues += sentenceValue[entry]\n",
    "\n",
    "    # Average value of a sentence from original text\n",
    "    average = (sumValues / len(sentenceValue))\n",
    "\n",
    "    return average\n",
    "\n",
    "\n",
    "def _generate_summary(sentences, sentenceValue, threshold):\n",
    "    sentence_count = 0\n",
    "    summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] >= (threshold):\n",
    "            summary += \" \" + sentence\n",
    "            sentence_count += 1\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def run_summarization(text):\n",
    "    # 1 Create the word frequency table\n",
    "    freq_table = _create_frequency_table(text)\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentence_scores = _score_sentences(sentences, freq_table)\n",
    "    threshold = _find_average_score(sentence_scores)\n",
    "    summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "test['label']='t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#data prep\n",
    "test=test.fillna(' ')\n",
    "train=train.fillna(' ')\n",
    "test['total']=test['title']+' '+test['author']+test['text']\n",
    "train['total']=train['title']+' '+train['author']+train['text']\n",
    "\n",
    "#tfidf\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "counts = count_vectorizer.fit_transform(train['total'].values)\n",
    "tfidf = transformer.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gray/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1466: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    }
   ],
   "source": [
    "targets = train['label'].values\n",
    "test_counts = count_vectorizer.transform(test['total'].values)\n",
    "test_tfidf = transformer.fit_transform(test_counts)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf, targets, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Lasso classifier on training set: 1.00\n",
      "Accuracy of Lasso classifier on test set: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gray/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "print('Accuracy of Lasso classifier on training set: {:.2f}'\n",
    "     .format(logreg.score(X_train, y_train)))\n",
    "print('Accuracy of Lasso classifier on test set: {:.2f}'\n",
    "     .format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e+00 9.99989048e-01 2.46202164e-05 ... 2.53000335e-07\n",
      " 1.02082068e-08 2.45538298e-05] [1 1 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = logreg.predict_proba(X_train)\n",
    "print(y_train_pred[:,1], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00108589 0.99996355 0.99999597 ... 0.22990449 0.9999998  0.55225337] [0 1 1 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "y_test_pred =logreg.predict_proba(X_test)\n",
    "print(y_test_pred[:,1], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gray/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id\n",
       "label      \n",
       "0      2603\n",
       "1      2597"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = train['label'].values\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(counts, targets)\n",
    "\n",
    "example_counts = count_vectorizer.transform(test['total'].values)\n",
    "predictions = logreg.predict(example_counts)\n",
    "pred=pd.DataFrame(predictions,columns=['label'])\n",
    "pred['id']=test['id']\n",
    "pred.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = pd.read_csv('/home/gray/Desktop/NeuralNews/Scraper-code/Second.csv')\n",
    "data_writing = pd.read_csv('/home/gray/Desktop/NeuralNews/Scraper-code/Second.csv')\n",
    "for i in range(len(prediction_data['Paragraph'])):\n",
    "    prediction_data['Paragraph'][i] = prediction_data['Paragraph'][i].strip(',;\\[]\\',\\'')\n",
    "    data_writing['Paragraph'][i] = data_writing['Paragraph'][i].strip(',;\\[]\\',\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_writing['label'] = 't'\n",
    "prediction_data = prediction_data.fillna(' ')\n",
    "data_writing = data_writing.fillna(' ') \n",
    "prediction_data['total'] = prediction_data['Title'] + prediction_data['Paragraph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words(\"english\"))\n",
    "\n",
    "for i in range(len(prediction_data['total'])):\n",
    "    li = \"\"\n",
    "    words = word_tokenize(prediction_data['total'][i])\n",
    "    for w in words:\n",
    "        #print(w,' ')\n",
    "        if w not in stopWords:\n",
    "            li = li +' '+ w\n",
    "    prediction_data['total'][i] = li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gray/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1466: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    }
   ],
   "source": [
    "pred_counts = count_vectorizer.transform(prediction_data['total'].values)\n",
    "pred_tfidf = transformer.fit_transform(pred_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logreg.predict_proba(pred_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data['label'] = predictions[:, [1]]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_writing['label'] = predictions[:, [1]]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  \",\n",
      "1  ', 'Oregon State Police troopers assisted local police at the protest. State police frequently helped police at the protests until early August, when they withdrew. ',\n",
      "2  \", 'Naidu, who was in the fourth car, escaped unhurt. ',\n",
      "3  \",\n",
      "4  Both have taken their toll, said Bauer in an interview. \",\n",
      "5  ', 'Both the mother and the fetus were under constant observation. ',\n",
      "6  You\\'ll get it in pouches,\" a peddler told India Today\\'s investigative reporter posing as staff of a wealthy businessman planning a drug party. probed the reporter. the reporter investigated.\"Yes. You\\'ll get in the shape of chalks. You\\'ll get 50 grams in one go,\" the peddler said.When India Today inquired about ganja, a peddler was found selling it some yards away. probed the reporter. the reporter asked a peddler there. You\\'ll get it in pouches,\" the peddler offered. asked the reporter. ',\n",
      "7  ', '“No party. ', '“(It was) the best I can do,” said Leclerc over the team radio. ', '“Valtteri was very very close, pushing.\n",
      "8  \", '\"I’m sure he’s going to be climbing up the rankings,\" Medvedev said of his Wolf.\n",
      "9  \",\n",
      "10  ', '\"The institute shall remain closed for one week. ', 'IIT employees will neither be allowed to enter nor leave the campus until further orders, except for an emergency.\n",
      "11  Action will be taken action against them, he added. The campaign needed to be fought politically and administratively, he added. ', 'Panchayats should be encouraged to propagate early testing and home quarantine, Jakhar said. ',\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gray/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "data_writing['Summary'] = 's'\n",
    "for i in range(len(data_writing['Title'])):\n",
    "    result = run_summarization(data_writing['Paragraph'][i])\n",
    "    data_writing['Summary'][i] = result\n",
    "    print(i, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                    \",\n",
       "1      ', 'Oregon State Police troopers assisted loc...\n",
       "2      \", 'Naidu, who was in the fourth car, escaped...\n",
       "3                                                    \",\n",
       "4      Both have taken their toll, said Bauer in an ...\n",
       "5      ', 'Both the mother and the fetus were under ...\n",
       "6      You\\'ll get it in pouches,\" a peddler told In...\n",
       "7      ', '“No party. ', '“(It was) the best I can d...\n",
       "8      \", '\"I’m sure he’s going to be climbing up th...\n",
       "9                                                    \",\n",
       "10     ', '\"The institute shall remain closed for on...\n",
       "11     Action will be taken action against them, he ...\n",
       "Name: Summary, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_writing['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_writing.to_csv('second_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_writing.to_json('second_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
