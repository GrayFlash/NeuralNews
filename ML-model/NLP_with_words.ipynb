{
<<<<<<< HEAD
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "NLP_with_words.ipynb",
      "provenance": []
=======
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_frequency_table(text_string) -> dict:\n",
    "\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text_string)\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    freqTable = dict()\n",
    "    for word in words:\n",
    "        word = ps.stem(word)\n",
    "        if word in stopWords:\n",
    "            continue\n",
    "        if word in freqTable:\n",
    "            freqTable[word] += 1\n",
    "        else:\n",
    "            freqTable[word] = 1\n",
    "\n",
    "    return freqTable\n",
    "\n",
    "\n",
    "def _score_sentences(sentences, freqTable) -> dict:\n",
    "    sentenceValue = dict()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        word_count_in_sentence = (len(word_tokenize(sentence)))\n",
    "        word_count_in_sentence_except_stop_words = 0\n",
    "        for wordValue in freqTable:\n",
    "            if wordValue in sentence.lower():\n",
    "                word_count_in_sentence_except_stop_words += 1\n",
    "                if sentence[:10] in sentenceValue:\n",
    "                    sentenceValue[sentence[:10]] += freqTable[wordValue]\n",
    "                else:\n",
    "                    sentenceValue[sentence[:10]] = freqTable[wordValue]\n",
    "\n",
    "        if sentence[:10] in sentenceValue:\n",
    "            sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] / word_count_in_sentence_except_stop_words\n",
    "    return sentenceValue\n",
    "\n",
    "\n",
    "def _find_average_score(sentenceValue) -> int:\n",
    "\n",
    "    sumValues = 0\n",
    "    for entry in sentenceValue:\n",
    "        sumValues += sentenceValue[entry]\n",
    "\n",
    "    # Average value of a sentence from original text\n",
    "    average = (sumValues / len(sentenceValue))\n",
    "\n",
    "    return average\n",
    "\n",
    "\n",
    "def _generate_summary(sentences, sentenceValue, threshold):\n",
    "    sentence_count = 0\n",
    "    summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] >= (threshold):\n",
    "            summary += \" \" + sentence\n",
    "            sentence_count += 1\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def run_summarization(text):\n",
    "    # 1 Create the word frequency table\n",
    "    freq_table = _create_frequency_table(text)\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentence_scores = _score_sentences(sentences, freq_table)\n",
    "    threshold = _find_average_score(sentence_scores)\n",
    "    summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "test['label']='t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#data prep\n",
    "test=test.fillna(' ')\n",
    "train=train.fillna(' ')\n",
    "test['total']=test['title']+' '+test['author']+test['text']\n",
    "train['total']=train['title']+' '+train['author']+train['text']\n",
    "\n",
    "#tfidf\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "counts = count_vectorizer.fit_transform(train['total'].values)\n",
    "tfidf = transformer.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gray/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1466: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
>>>>>>> 74011c1db3dd8d212282061934950d45756aa6f6
    }
  },
<<<<<<< HEAD
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgpQzNgIt_i1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "04586300-d97b-4029-f26f-6a190e42bf44"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrAAFffXt_i5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _create_frequency_table(text_string) -> dict:\n",
        "\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text_string)\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    freqTable = dict()\n",
        "    for word in words:\n",
        "        word = ps.stem(word)\n",
        "        if word in stopWords:\n",
        "            continue\n",
        "        if word in freqTable:\n",
        "            freqTable[word] += 1\n",
        "        else:\n",
        "            freqTable[word] = 1\n",
        "\n",
        "    return freqTable\n",
        "\n",
        "\n",
        "def _score_sentences(sentences, freqTable) -> dict:\n",
        "    sentenceValue = dict()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        word_count_in_sentence = (len(word_tokenize(sentence)))\n",
        "        word_count_in_sentence_except_stop_words = 0\n",
        "        for wordValue in freqTable:\n",
        "            if wordValue in sentence.lower():\n",
        "                word_count_in_sentence_except_stop_words += 1\n",
        "                if sentence[:10] in sentenceValue:\n",
        "                    sentenceValue[sentence[:10]] += freqTable[wordValue]\n",
        "                else:\n",
        "                    sentenceValue[sentence[:10]] = freqTable[wordValue]\n",
        "\n",
        "        if sentence[:10] in sentenceValue:\n",
        "            sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] / word_count_in_sentence_except_stop_words\n",
        "    return sentenceValue\n",
        "\n",
        "\n",
        "def _find_average_score(sentenceValue) -> int:\n",
        "\n",
        "    sumValues = 0\n",
        "    for entry in sentenceValue:\n",
        "        sumValues += sentenceValue[entry]\n",
        "\n",
        "    # Average value of a sentence from original text\n",
        "    average = (sumValues / len(sentenceValue))\n",
        "\n",
        "    return average\n",
        "\n",
        "\n",
        "def _generate_summary(sentences, sentenceValue, threshold):\n",
        "    sentence_count = 0\n",
        "    summary = ''\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] >= (threshold):\n",
        "            summary += \" \" + sentence\n",
        "            sentence_count += 1\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "def run_summarization(text):\n",
        "    # 1 Create the word frequency table\n",
        "    freq_table = _create_frequency_table(text)\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentence_scores = _score_sentences(sentences, freq_table)\n",
        "    threshold = _find_average_score(sentence_scores)\n",
        "    summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n",
        "\n",
        "    return summary\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WGf53SXt_i-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=pd.read_csv('train.csv')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27tviDcit_jA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "#data prep\n",
        "train=train.fillna(' ')\n",
        "train['total']=train['title']+' '+train['author']+train['text']\n",
        "\n",
        "#tfidf\n",
        "transformer = TfidfTransformer(smooth_idf=False)\n",
        "count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tllawy8Q0AQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "7efd35d6-16e2-4993-8d3e-7a47d057c4b2"
      },
      "source": [
        "stopWords = set(stopwords.words(\"english\"))\n",
        "\n",
        "for i in range(len(train['total'])):\n",
        "    li = \"\"\n",
        "    words = word_tokenize(train['total'][i])\n",
        "    for w in words:\n",
        "        #print(w,' ')\n",
        "        if w not in stopWords:\n",
        "            li = li +' '+ w\n",
        "    train['total'][i] = li"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
=======
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gray/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Lasso classifier on training set: 1.00\n",
      "Accuracy of Lasso classifier on test set: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "print('Accuracy of Lasso classifier on training set: {:.2f}'\n",
    "     .format(logreg.score(X_train, y_train)))\n",
    "print('Accuracy of Lasso classifier on test set: {:.2f}'\n",
    "     .format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e+00 9.99989048e-01 2.46202164e-05 ... 2.53000335e-07\n",
      " 1.02082068e-08 2.45538298e-05] [1 1 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = logreg.predict_proba(X_train)\n",
    "print(y_train_pred[:,1], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00108589 0.99996355 0.99999597 ... 0.22990449 0.9999998  0.55225337] [0 1 1 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "y_test_pred =logreg.predict_proba(X_test)\n",
    "print(y_test_pred[:,1], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
>>>>>>> 74011c1db3dd8d212282061934950d45756aa6f6
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL4YS-lCt_jC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counts = count_vectorizer.fit_transform(train['total'].values)\n",
        "tfidf = transformer.fit_transform(counts)\n",
        "targets = train['label'].values\n",
        "# test_counts = count_vectorizer.transform(test['total'].values)\n",
        "# test_tfidf = transformer.fit_transform(test_counts)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf, targets, random_state=0)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEyhefvit_jG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "3a1cb9a8-beb0-470a-de3e-45fdfe70739e"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(C=1e5)\n",
        "logreg.fit(X_train, y_train)\n",
        "print('Accuracy of Lasso classifier on training set: {:.2f}'\n",
        "     .format(logreg.score(X_train, y_train)))\n",
        "print('Accuracy of Lasso classifier on test set: {:.2f}'\n",
        "     .format(logreg.score(X_test, y_test)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Lasso classifier on training set: 1.00\n",
            "Accuracy of Lasso classifier on test set: 0.98\n"
          ],
          "name": "stdout"
        }
      ]
<<<<<<< HEAD
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_YdtRpTt_jK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9ab5f4d0-05d4-41ba-9d84-c551bb30332c"
      },
      "source": [
        "y_train_pred = logreg.predict_proba(X_train)\n",
        "print(y_train_pred[:,1], y_train)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.00000000e+00 9.99978523e-01 2.45793885e-05 ... 6.15956923e-08\n",
            " 8.78118218e-09 1.87527342e-05] [1 1 0 ... 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0te-zV0yt_jN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "039591f3-1c31-4bd8-bc5c-346a87aa53c0"
      },
      "source": [
        "y_test_pred =logreg.predict_proba(X_test)\n",
        "print(y_test_pred[:,1], y_test)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.15535595e-04 9.99960924e-01 9.99933290e-01 ... 8.24378158e-02\n",
            " 9.99997843e-01 4.30508866e-01] [0 1 1 ... 0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
=======
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = train['label'].values\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(counts, targets)\n",
    "\n",
    "example_counts = count_vectorizer.transform(test['total'].values)\n",
    "predictions = logreg.predict(example_counts)\n",
    "pred=pd.DataFrame(predictions,columns=['label'])\n",
    "pred['id']=test['id']\n",
    "pred.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = pd.read_csv('/home/gray/git_folders/NeuralNews/Scraper-code/ThirdSetArticles.csv')\n",
    "data_writing = pd.read_csv('/home/gray/git_folders/NeuralNews/Scraper-code/ThirdSetArticles.csv')\n",
    "for i in range(len(prediction_data['Paragraph'])):\n",
    "    prediction_data['Paragraph'][i] = prediction_data['Paragraph'][i].strip(',;\\[]\\',\\'')\n",
    "    data_writing['Paragraph'][i] = data_writing['Paragraph'][i].strip(',;\\[]\\',\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_writing['label'] = 't'\n",
    "prediction_data = prediction_data.fillna(' ')\n",
    "data_writing = data_writing.fillna(' ') \n",
    "prediction_data['total'] = prediction_data['Title'] + prediction_data['Paragraph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words(\"english\"))\n",
    "\n",
    "for i in range(len(prediction_data['total'])):\n",
    "    li = \"\"\n",
    "    words = word_tokenize(prediction_data['total'][i])\n",
    "    for w in words:\n",
    "        #print(w,' ')\n",
    "        if w not in stopWords:\n",
    "            li = li +' '+ w\n",
    "    prediction_data['total'][i] = li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gray/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1466: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    }
   ],
   "source": [
    "pred_counts = count_vectorizer.transform(prediction_data['total'].values)\n",
    "pred_tfidf = transformer.fit_transform(pred_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logreg.predict_proba(pred_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data['label'] = predictions[:, [1]]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_writing['label'] = predictions[:, [1]]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  “We want to show all other countries that this is what the Chinese government will do if you bully any of our companies, so don’t follow what the U.S. is doing,” the source said. China’s commerce ministry, which published revisions to the tech export control list, did not respond to a request for comment.\n",
      "1 \n",
      "2  “There is a great group of people here,” he said, dressed as a “Burning Legion” demon.\n",
      "3 \n",
      "4  Ant also declined to comment. Under them are joint global coordinators, and on the bottom rung are joint lead managers.\n",
      "5  Huawei rejects the charges.\n",
      "6  Amazon closed down 2.2%.\n",
      "7  Krezic declined to speak to Reuters. There was no immediate comment from Tesla and Ford.\n",
      "8  It was created in 2017.\n",
      "9  Etsy, Teradyne and Catalent have a combined stock market value of about $40 billion.\n"
     ]
>>>>>>> 74011c1db3dd8d212282061934950d45756aa6f6
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLBzwecOt_jP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "9e3cc25c-03a9-43cb-8e46-45e3224d4756"
      },
      "source": [
        "targets = train['label'].values\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(counts, targets)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIWcLgsCt_jR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction_data = pd.read_csv('First few Articles.csv')\n",
        "data_writing = pd.read_csv('First few Articles.csv')\n",
        "for i in range(len(prediction_data['Paragraph'])):\n",
        "    prediction_data['Paragraph'][i] = prediction_data['Paragraph'][i].strip(',;\\[]\\',\\'')\n",
        "    data_writing['Paragraph'][i] = data_writing['Paragraph'][i].strip(',;\\[]\\',\\'')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iuQiMjqt_jU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_writing['label'] = 't'\n",
        "prediction_data = prediction_data.fillna(' ')\n",
        "data_writing = data_writing.fillna(' ') \n",
        "prediction_data['total'] = prediction_data['Title'] + prediction_data['Paragraph']"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChrkFBoSt_jW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopWords = set(stopwords.words(\"english\"))\n",
        "\n",
        "for i in range(len(prediction_data['total'])):\n",
        "    li = \"\"\n",
        "    words = word_tokenize(prediction_data['total'][i])\n",
        "    for w in words:\n",
        "        #print(w,' ')\n",
        "        if w not in stopWords:\n",
        "            li = li +' '+ w\n",
        "    prediction_data['total'][i] = li"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
<<<<<<< HEAD
      "cell_type": "code",
      "metadata": {
        "id": "B-S2Vpdnt_jX",
        "colab_type": "code",
        "colab": {},
        "outputId": "052f5f6c-2c15-4b27-d23d-90b3f4458edd"
      },
      "source": [
        "pred_counts = count_vectorizer.transform(prediction_data['total'].values)\n",
        "pred_tfidf = transformer.fit_transform(pred_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/gray/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1466: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  idf = np.log(n_samples / df) + 1\n"
          ],
          "name": "stderr"
        }
=======
     "data": {
      "text/plain": [
       "0     “We want to show all other countries that thi...\n",
       "1                                                     \n",
       "2     “There is a great group of people here,” he s...\n",
       "3                                                     \n",
       "4     Ant also declined to comment. Under them are ...\n",
       "5                          Huawei rejects the charges.\n",
       "6                             Amazon closed down 2.2%.\n",
       "7     Krezic declined to speak to Reuters. There wa...\n",
       "8                              It was created in 2017.\n",
       "9     Etsy, Teradyne and Catalent have a combined s...\n",
       "Name: Summary, dtype: object"
>>>>>>> 74011c1db3dd8d212282061934950d45756aa6f6
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQu8vCRnt_jZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = logreg.predict_proba(pred_counts)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOjkH0YXt_jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction_data['label'] = predictions[:, [1]]*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfbCdeXlt_jd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_writing['label'] = predictions[:, [1]]*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyaK2C6Xt_jf",
        "colab_type": "code",
        "colab": {},
        "outputId": "fd8c65a2-c047-4f98-f95a-4561a63563ae"
      },
      "source": [
        "data_writing['Summary'] = 's'\n",
        "for i in range(len(data_writing['Title'])):\n",
        "    result = run_summarization(data_writing['Paragraph'][i])\n",
        "    data_writing['Summary'][i] = result\n",
        "    print(i, result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0  \",\n",
            "1  ', 'Oregon State Police troopers assisted local police at the protest. State police frequently helped police at the protests until early August, when they withdrew. ',\n",
            "2  \", 'Naidu, who was in the fourth car, escaped unhurt. ',\n",
            "3  \",\n",
            "4  Both have taken their toll, said Bauer in an interview. \",\n",
            "5  ', 'Both the mother and the fetus were under constant observation. ',\n",
            "6  You\\'ll get it in pouches,\" a peddler told India Today\\'s investigative reporter posing as staff of a wealthy businessman planning a drug party. probed the reporter. the reporter investigated.\"Yes. You\\'ll get in the shape of chalks. You\\'ll get 50 grams in one go,\" the peddler said.When India Today inquired about ganja, a peddler was found selling it some yards away. probed the reporter. the reporter asked a peddler there. You\\'ll get it in pouches,\" the peddler offered. asked the reporter. ',\n",
            "7  ', '“No party. ', '“(It was) the best I can do,” said Leclerc over the team radio. ', '“Valtteri was very very close, pushing.\n",
            "8  \", '\"I’m sure he’s going to be climbing up the rankings,\" Medvedev said of his Wolf.\n",
            "9  \",\n",
            "10  ', '\"The institute shall remain closed for one week. ', 'IIT employees will neither be allowed to enter nor leave the campus until further orders, except for an emergency.\n",
            "11  Action will be taken action against them, he added. The campaign needed to be fought politically and administratively, he added. ', 'Panchayats should be encouraged to propagate early testing and home quarantine, Jakhar said. ',\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/home/gray/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iemOizRet_jh",
        "colab_type": "code",
        "colab": {},
        "outputId": "9240b424-fcba-4d56-f651-bc8184eada71"
      },
      "source": [
        "data_writing['Summary']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                    \",\n",
              "1      ', 'Oregon State Police troopers assisted loc...\n",
              "2      \", 'Naidu, who was in the fourth car, escaped...\n",
              "3                                                    \",\n",
              "4      Both have taken their toll, said Bauer in an ...\n",
              "5      ', 'Both the mother and the fetus were under ...\n",
              "6      You\\'ll get it in pouches,\" a peddler told In...\n",
              "7      ', '“No party. ', '“(It was) the best I can d...\n",
              "8      \", '\"I’m sure he’s going to be climbing up th...\n",
              "9                                                    \",\n",
              "10     ', '\"The institute shall remain closed for on...\n",
              "11     Action will be taken action against them, he ...\n",
              "Name: Summary, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWPdv0A5t_jk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_writing.to_csv('second_dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a2ufN0mt_jm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_writing.to_json('second_dataset.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0smp6fajt_jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZYV9ULXt_jq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
<<<<<<< HEAD
  ]
}
=======
   ],
   "source": [
    "data_writing['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_writing.to_csv('third_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_writing.to_json('third_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
>>>>>>> 74011c1db3dd8d212282061934950d45756aa6f6
